# 深度学习笔记
[TOC]
### 全连接层的局限性
* 图像在同一列邻近的像素在这个向量中可能相距较远。它们构成的模式可能难以被模型识别
* 对于大尺寸的输入图像，使用全连接层容易导致模型过大。

### 卷积层的优势
* 卷积层可以保留图像输入形状
* 卷积层通过滑动窗口将同一卷积核与不同位置的输入重复计算，从而避免参数尺寸过大。

### 卷积核与输出的通道数
一个卷积核对图像进行运算，输出一个通道的特征图，即有几个卷积核就会输出几个通道。每个卷积核提取一种特征，多的卷积核（即多的通道）就代表着更多的特征。

### 欠拟合、过拟合
* 欠拟合是指训练误差和泛化误差都比较大，即模型太简单不能很好的拟合数据
* 过拟合是指训练误差小而泛化误差大，模型泛化能力差。主要原因是模型复杂、训练数据较少

**过拟合解决方法**
1. **权值衰减:** 即在 loss 中添加L2范数正则项，惩罚绝对值过大的权值。权值衰减有抑制拟合的效果，因此可以缓解过拟合，但也可能造成欠拟合
2. **Dropout:** 对于神经元的输出，以一定的概率 p 使其清零，保留下来的输出除以 1-p 进行拉伸，这样可以保证该层的输出的期望值不变
3. **增加训练数据集** 

### 全连接层和 1x1 的卷积层
1x1 的卷积层相当于全连接层

### VGG 模型
* VGG 是通过叠加基础块 VGG block 的个数，来构建复杂的模型或简单的模型。
* VGG block 的构成是几个 3x3 且 padding 为 1 的卷积层叠加，再加一个 2x2 的最大池化层

### 深度学习优化方法中遇到的问题
深度学习中基于梯度下降的优化方法会遇到以下问题：
1. 局部最小值
2. 鞍点
    * 鞍点在单变量函数中，为二阶导为0的点；在多变量函数中，为函数对所有自变量一阶导为0，且Hessian矩阵特征值有正有负的点
3. 梯度消失
    * 即损失函数的某部分十分平滑，梯度接近0，则要经过很长时间梯度才能有明显变化

### 凸优化
* 凸集

### 随机梯度下降
* **梯度下降：** 是对整个数据集的所有样本的损失计算梯度均值，然后更新参数。这样时间复杂度较高，且样本较为相似时，效率较低
* **随机梯度下降：** 是对单个样本的损失计算梯度后即更新参数
* **小批量随机梯度下降**：


### GAN
* **原理：** 对生成的假的图像和真实的图像通过一个分类器进行分类，如果能欺骗一个分类器的话，说明生成的效果很好
* **two sample test**
* **结构**
![GAN](./所有插图/GAN_结构.png)
